{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71993902-0e79-4b54-862c-6a3cfca42535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------+----+------+-----+-------------------+-------------------+\n|meter_point_id| ssc|profile_class| tpr|eac_id|  eac|effective_from_date|  effective_to_date|\n+--------------+----+-------------+----+------+-----+-------------------+-------------------+\n|             1|   A|            3|TPR1|     1| 62.0|2023-04-08 00:00:00|2023-06-21 00:00:00|\n|             1|   B|            2|TPR3|     2|42.81|2023-03-30 00:00:00|2023-04-15 00:00:00|\n|             4|   B|            4|TPR3|     3|91.27|2023-03-12 00:00:00|2023-05-01 00:00:00|\n|             5|   C|            2|TPR1|     4|26.04|2023-01-03 00:00:00|2023-02-18 00:00:00|\n|             1|   C|         NULL|TPR3|     5|16.16|2023-04-04 00:00:00|2023-04-08 00:00:00|\n|             1|NULL|         NULL|TPR2|     6|18.36|2023-02-21 00:00:00|2023-05-05 00:00:00|\n|             5|   A|         NULL|TPR2|     7|58.27|2023-02-07 00:00:00|2023-05-10 00:00:00|\n|             3|NULL|            2|TPR2|     8|82.78|2023-02-13 00:00:00|2023-02-26 00:00:00|\n|             5|   B|            1|TPR2|     9|33.93|2023-03-21 00:00:00|2023-05-18 00:00:00|\n|             3|   C|            4|TPR1|    10|77.92|2023-01-17 00:00:00|2023-04-05 00:00:00|\n|             3|NULL|            4|TPR1|    11|26.47|2023-01-29 00:00:00|2023-04-28 00:00:00|\n|             2|   B|            2|TPR3|    12|21.28|2023-02-12 00:00:00|2023-02-25 00:00:00|\n|             4|NULL|            2|TPR1|    13|66.77|2023-02-15 00:00:00|2023-04-08 00:00:00|\n|             4|   C|            1|TPR3|    14| 11.1|2023-01-20 00:00:00|2023-02-17 00:00:00|\n|             4|   B|            1|TPR2|    15|16.93|2023-01-30 00:00:00|2023-04-21 00:00:00|\n|             3|   C|            3|TPR2|    16|93.05|2023-01-15 00:00:00|2023-01-22 00:00:00|\n|             3|   C|            1|TPR3|    17|99.59|2023-03-07 00:00:00|2023-06-02 00:00:00|\n|             4|   A|            2|TPR2|    18|91.82|2023-01-16 00:00:00|2023-03-10 00:00:00|\n|             5|   A|         NULL|TPR1|    19|58.77|2023-03-07 00:00:00|2023-05-06 00:00:00|\n|             4|   B|            1|TPR2|    20|88.06|2023-02-23 00:00:00|2023-02-26 00:00:00|\n+--------------+----+-------------+----+------+-----+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import avg, count, col, last, first, lit\n",
    "import pandas as pd\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SampleDataCreation\").getOrCreate()\n",
    " \n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"meter_point_id\", IntegerType(), True),\n",
    "    StructField(\"ssc\", StringType(), True),\n",
    "    StructField(\"profile_class\", IntegerType(), True),\n",
    "    StructField(\"tpr\", StringType(), True),\n",
    "    StructField(\"eac_id\", IntegerType(), True),\n",
    "    StructField(\"eac\", FloatType(), True),\n",
    "    StructField(\"effective_from_date\", TimestampType(), True),\n",
    "    StructField(\"effective_to_date\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# Generate sample data\n",
    "sample_data = []\n",
    "num_records = 20\n",
    "start_date = datetime(2023, 1, 1)\n",
    "\n",
    "for i in range(num_records):\n",
    "    meter_point_id = random.randint(1, 5)\n",
    "    ssc = random.choice(['A', 'B', 'C', None])\n",
    "    profile_class = random.choice([1, 2, 3, 4, None])\n",
    "    tpr = random.choice(['TPR1', 'TPR2', 'TPR3'])\n",
    "    eac_id = i + 1\n",
    "    eac = round(random.uniform(10.0, 100.0), 2)\n",
    "    effective_from_date = start_date + timedelta(days=random.randint(0, 100))\n",
    "    effective_to_date = effective_from_date + timedelta(days=random.randint(1, 100))\n",
    "    sample_data.append((meter_point_id, ssc, profile_class, tpr, eac_id, eac, effective_from_date, effective_to_date))\n",
    "\n",
    "# Create DataFrame from sample data\n",
    "df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "# Show sample data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f59c35-b180-45a2-b7ff-c4d95802f4cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_data(df) -> df:\n",
    "    \"\"\"\n",
    "    Process EACs.\n",
    "    \"\"\"\n",
    "    keep_cols = [\n",
    "        'meter_point_id', 'ssc', 'profile_class', 'tpr', 'eac_id', 'eac', \n",
    "        'effective_from_date', 'effective_to_date'\n",
    "    ]\n",
    "    df = df.select(keep_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e950809f-4d2a-4092-bb2e-6df0c8352af8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, a WindowSpec is defined. A WindowSpec defines the partitioning and ordering of rows for window functions.\n",
    "partitionBy(\"meter_point_id\") partitions the data by the \"meter_point_id\" column. This means that window functions will operate on partitions of rows that share the same \"meter_point_id\" value.\n",
    "orderBy(\"effective_from_date\") orders the rows within each partition by the \"effective_from_date\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e61b21-7d04-410c-9bb4-5a9c52b86c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-------------+\n|meter_point_id|ssc|profile_class|\n+--------------+---+-------------+\n|             1|  B|         NULL|\n|             1|  B|            2|\n|             1|  C|         NULL|\n|             1|  A|            3|\n|             2|  B|            2|\n|             3|  C|            3|\n|             3|  C|            4|\n|             3|  C|            4|\n|             3|  C|            2|\n|             3|  C|            1|\n|             4|  A|            2|\n|             4|  C|            1|\n|             4|  B|            1|\n|             4|  B|            2|\n|             4|  B|            1|\n|             4|  B|            4|\n|             5|  C|            2|\n|             5|  A|         NULL|\n|             5|  A|         NULL|\n|             5|  B|            1|\n+--------------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Assuming df is a Spark DataFrame\n",
    "df_filled = df.orderBy(['meter_point_id', 'effective_from_date']) \\\n",
    "              .select(['meter_point_id', 'ssc', 'profile_class'])\n",
    "\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22201b2e-8381-48d7-b0f9-4711f18225cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Forward filling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e679f7c-98e3-4c35-8b18-81cd146dac3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-------------+\n|meter_point_id|ssc|profile_class|\n+--------------+---+-------------+\n|             1|  B|         NULL|\n|             1|  B|            2|\n|             1|  C|         NULL|\n|             1|  A|            3|\n|             2|  B|            2|\n|             3|  C|            3|\n|             3|  C|            4|\n|             3|  C|            4|\n|             3|  C|            2|\n|             3|  C|            1|\n|             4|  A|            2|\n|             4|  C|            1|\n|             4|  B|            1|\n|             4|  B|            2|\n|             4|  B|            1|\n|             4|  B|            4|\n|             5|  C|            2|\n|             5|  A|         NULL|\n|             5|  A|         NULL|\n|             5|  B|            1|\n+--------------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.rowsBetween(float('-inf'),0) ## specifying direction of filling\n",
    "\n",
    " # defining the forward-filled column\n",
    "filled_column_1 = last(df_filled['ssc'], ignorenulls=True).over(window)\n",
    "\n",
    "\n",
    "# replacing the columns with forward-filled columns\n",
    "df=df_filled.withColumn('ssc', filled_column_1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5df723-fd31-4171-b4b8-b47038e1ecf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##BACKWARD FILLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94ff711-4236-44da-a412-e102ac03d93f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-------------+\n|meter_point_id|ssc|profile_class|\n+--------------+---+-------------+\n|             1|  B|         NULL|\n|             1|  B|            2|\n|             1|  C|         NULL|\n|             1|  A|            3|\n|             2|  B|            2|\n|             3|  C|            3|\n|             3|  C|            4|\n|             3|  C|            4|\n|             3|  C|            2|\n|             3|  C|            1|\n|             4|  A|            2|\n|             4|  C|            1|\n|             4|  B|            1|\n|             4|  B|            2|\n|             4|  B|            1|\n|             4|  B|            4|\n|             5|  C|            2|\n|             5|  A|         NULL|\n|             5|  A|         NULL|\n|             5|  B|            1|\n+--------------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "window = Window.rowsBetween(0,float('inf')) ## specifying direction of filling\n",
    "\n",
    "\n",
    " # defining the forward-filled column\n",
    "filled_column_1 = first(df['ssc'], ignorenulls=True).over(window)\n",
    "\n",
    "\n",
    "# replacing the columns with forward-filled columns\n",
    "df=df.withColumn('ssc', filled_column_1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263b2f55-cbba-41e1-8e23-90625db1ddd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+-------------+\n|meter_point_id|ssc|profile_class|\n+--------------+---+-------------+\n|             1|  B|         NULL|\n|             1|  B|            2|\n|             1|  C|         NULL|\n|             1|  A|            3|\n|             2|  B|            2|\n|             3|  C|            3|\n|             3|  C|            4|\n|             3|  C|            4|\n|             3|  C|            2|\n|             3|  C|            1|\n|             4|  A|            2|\n|             4|  C|            1|\n|             4|  B|            1|\n|             4|  B|            2|\n|             4|  B|            1|\n|             4|  B|            4|\n|             5|  C|            2|\n|             5|  A|         NULL|\n|             5|  A|         NULL|\n|             5|  B|            1|\n+--------------+---+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, use fillna to fill remaining null values in other columns\n",
    "for col in df_filled.columns:\n",
    "    # Check if the column exists in 'df' before filling\n",
    "    if col in df.columns:\n",
    "        # Extract the scalar value from df_filled and fillna\n",
    "        fill_value = df_filled.select(col).first()[col]\n",
    "        if fill_value is not None:\n",
    "            df = df.fillna(fill_value, subset=[col])\n",
    "\n",
    "# Show the DataFrame after filling remaining null values\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98241153-d16b-41f1-9aff-5478cfb4dd7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>meter_point_id</th><th>ssc</th><th>profile_class</th></tr></thead><tbody><tr><td>1</td><td>B</td><td>null</td></tr><tr><td>1</td><td>B</td><td>2</td></tr><tr><td>1</td><td>C</td><td>null</td></tr><tr><td>1</td><td>A</td><td>3</td></tr><tr><td>2</td><td>B</td><td>2</td></tr><tr><td>3</td><td>C</td><td>3</td></tr><tr><td>3</td><td>C</td><td>4</td></tr><tr><td>3</td><td>C</td><td>4</td></tr><tr><td>3</td><td>C</td><td>2</td></tr><tr><td>3</td><td>C</td><td>1</td></tr><tr><td>4</td><td>A</td><td>2</td></tr><tr><td>4</td><td>C</td><td>1</td></tr><tr><td>4</td><td>B</td><td>1</td></tr><tr><td>4</td><td>B</td><td>2</td></tr><tr><td>4</td><td>B</td><td>1</td></tr><tr><td>4</td><td>B</td><td>4</td></tr><tr><td>5</td><td>C</td><td>2</td></tr><tr><td>5</td><td>A</td><td>null</td></tr><tr><td>5</td><td>A</td><td>null</td></tr><tr><td>5</td><td>B</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "B",
         null
        ],
        [
         1,
         "B",
         2
        ],
        [
         1,
         "C",
         null
        ],
        [
         1,
         "A",
         3
        ],
        [
         2,
         "B",
         2
        ],
        [
         3,
         "C",
         3
        ],
        [
         3,
         "C",
         4
        ],
        [
         3,
         "C",
         4
        ],
        [
         3,
         "C",
         2
        ],
        [
         3,
         "C",
         1
        ],
        [
         4,
         "A",
         2
        ],
        [
         4,
         "C",
         1
        ],
        [
         4,
         "B",
         1
        ],
        [
         4,
         "B",
         2
        ],
        [
         4,
         "B",
         1
        ],
        [
         4,
         "B",
         4
        ],
        [
         5,
         "C",
         2
        ],
        [
         5,
         "A",
         null
        ],
        [
         5,
         "A",
         null
        ],
        [
         5,
         "B",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "meter_point_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ssc",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "profile_class",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d20f0ed-e01e-4929-be17-2c829db51b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "This module provides various utilities for users to interact with the rest of Databricks.\n",
       "  <h3></h3><b>credentials: DatabricksCredentialUtils</b> -> Utilities for interacting with credentials within notebooks<br /><b>data: DataUtils</b> -> Utilities for understanding and interacting with datasets (EXPERIMENTAL)<br /><b>fs: DbfsUtils</b> -> Manipulates the Databricks filesystem (DBFS) from the console<br /><b>jobs: JobsUtils</b> -> Utilities for leveraging jobs features<br /><b>library: LibraryUtils</b> -> Utilities for session isolated libraries<br /><b>meta: MetaUtils</b> -> Methods to hook into the compiler (EXPERIMENTAL)<br /><b>notebook: NotebookUtils</b> -> Utilities for the control flow of a notebook (EXPERIMENTAL)<br /><b>preview: Preview</b> -> Utilities under preview category<br /><b>secrets: SecretUtils</b> -> Provides utilities for leveraging secrets within notebooks<br /><b>widgets: WidgetsUtils</b> -> Methods to create and get bound value of input widgets inside notebooks<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17788edd-cd85-476b-b6f2-35157a792137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89567e20-e607-433b-82ae-534c3a83fc31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs('dbfs/FileStore/sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4857e4-c022-4dcb-8e89-3c5efd5f0132",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">\n",
       "The notebook module.\n",
       "  <h3></h3><b>exit(value: String): void</b> -> This method lets you exit a notebook with a value<br /><b>run(path: String, timeoutSeconds: int, arguments: Map): String</b> -> This method runs a notebook and returns its exit value<br /><br /></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.notebook.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "002a1e23-f7c1-42c3-94d7-e2786f62f6b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Window Specification for Forward Fill:\n",
    "Window.orderBy(\"id\"): Creates a window specification that orders rows by the id column.\n",
    ".rowsBetween(Window.unboundedPreceding, 0): Defines the window frame to include all rows from the start of the partition (Window.unboundedPreceding) up to the current row (0). This means the window includes all preceding rows and the current row, which is necessary for forward filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c8525fe-bbec-48b8-8ec3-3e67c84ca81c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply forward fill using last function ignoring nulls\n",
    "df_ffill = df.withColumn(\"ffilled_value\", F.last(\"value\", ignorenulls=True).over(window_spec_ffill))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7ffe53-fb7b-4734-8062-c3e8263c9ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Apply forward fill using last function ignoring nulls\n",
    "df_ffill = df.withColumn(\"ffilled_value\", F.last(\"value\", ignorenulls=True).over(window_spec_ffill))\n",
    "#Apply Forward Fill:\n",
    "df.withColumn(\"ffilled_value\", ...) :\n",
    "\n",
    " Adds a new column ffilled_value to the DataFrame df.\n",
    "F.last(\"value\", ignorenulls=True).over(window_spec_ffill):\n",
    "\n",
    " Uses the last function to get the last non-null value in the specified window (window_spec_ffill), effectively performing a forward fill.\n",
    "F.last(\"value\", ignorenulls=True): Returns the last non-null value from the column value within the window.\n",
    ".over(window_spec_ffill): Applies the window specification defined earlier for forward fill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "742ad33b-0fa5-41d9-842b-98336ed78b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Window Specification for Backward Fill:\n",
    "Window.orderBy(\"id\"): Again, creates a window specification that orders rows by the id column.\n",
    ".rowsBetween(0, Window.unboundedFollowing): Defines the window frame to include all rows from the current row (0) to the end of the partition (Window.unboundedFollowing). This means the window includes the current row and all subsequent rows, which is necessary for backward filling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb208c4-17bd-40cd-93e8-4df7ebce773c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply backward fill using first function ignoring nulls\n",
    "df_filled = df_ffill.withColumn(\"filled_value\", F.first(\"ffilled_value\", ignorenulls=True).over(window_spec_bfill))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63e1aa7-fd57-4e47-a7a9-e5044f2b4c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FillNA\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, None),\n",
    "    (2, None),\n",
    "    (3, \"a\"),\n",
    "    (4, None),\n",
    "    (5, \"b\"),\n",
    "    (6, None)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7461ff-7a1f-4a46-a655-b60c21c95d61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apply Backward Fill:\n",
    "df_ffill.withColumn(\"filled_value\", ...): Adds a new column filled_value to the DataFrame df_ffill (which already includes the forward-filled values).\n",
    "F.first(\"ffilled_value\", ignorenulls=True).over(window_spec_bfill): Uses the first function to get the first non-null value in the specified window (window_spec_bfill), effectively performing a backward fill.\n",
    "F.first(\"ffilled_value\", ignorenulls=True): Returns the first non-null value from the column ffilled_value within the window.\n",
    ".over(window_spec_bfill): Applies the window specification defined earlier for backward fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb3c5836-52c3-45ee-9a96-c9c48c229906",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a45eeb-b902-41bd-a17b-59c5eedc1291",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Show the Result:\n",
    "df_filled.show(): Displays the content of the resulting DataFrame df_filled with the columns ffilled_value (forward-filled values) and filled_value (forward and backward-filled values).\n",
    "Summary\n",
    "Define Window Specifications:\n",
    "\n",
    "window_spec_ffill: For forward filling, includes all rows up to the current row.\n",
    "window_spec_bfill: For backward filling, includes the current row and all subsequent rows.\n",
    "Apply Forward Fill:\n",
    "\n",
    "Use F.last with ignorenulls=True to get the last non-null value within the forward fill window.\n",
    "Apply Backward Fill:\n",
    "\n",
    "Use F.first with ignorenulls=True to get the first non-null value within the backward fill window.\n",
    "Display the Result:\n",
    "\n",
    "Show the DataFrame with the new columns showing the forward and backward filled values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41623f13-2505-4590-8beb-9d0b9aa9759f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id|value|value2|\n+---+-----+------+\n|  1| NULL|  NULL|\n|  2| NULL|     2|\n|  3|    a|     3|\n|  4| NULL|     1|\n|  5|    b|  NULL|\n|  6| NULL|     2|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FillNA\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (1, None, None),\n",
    "    (2, None, 2),\n",
    "    (3, \"a\", 3),\n",
    "    (4, None, 1),\n",
    "    (5, \"b\", None),\n",
    "    (6, None, 2)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\", \"value2\"])\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74526e73-b811-4d27-b741-b2cecc800e48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id|value|value2|\n+---+-----+------+\n|  1| NULL|  NULL|\n|  2| NULL|     2|\n|  3|    a|     3|\n|  4|    a|     1|\n|  5|    b|     1|\n|  6|    b|     2|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Define window specifications for forward fill\n",
    "window_spec_ffill = Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Apply forward fill using last function ignoring nulls\n",
    "df_ffill = df.withColumn(\"value\", F.last(\"value\", ignorenulls=True).over(window_spec_ffill)) \\\n",
    "             .withColumn(\"value2\", F.last(\"value2\", ignorenulls=True).over(window_spec_ffill))\n",
    "\n",
    "df_ffill.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b242cc96-4122-4324-baa6-8d009f799bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id|value|value2|\n+---+-----+------+\n|  1|    a|     2|\n|  2|    a|     2|\n|  3|    a|     3|\n|  4|    a|     1|\n|  5|    b|     1|\n|  6|    b|     2|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define window specifications for backward fill\n",
    "window_spec_bfill = Window.orderBy(\"id\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "# Apply backward fill using first function ignoring nulls\n",
    "df_bfill = df_ffill.withColumn(\"value\", F.first(\"value\", ignorenulls=True).over(window_spec_bfill)) \\\n",
    "             .withColumn(\"value2\", F.first(\"value2\", ignorenulls=True).over(window_spec_bfill))\n",
    "\n",
    "df_bfill.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c076fcf-dc69-41a7-9780-26a31116b826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id|value|value2|\n+---+-----+------+\n|  1|    a|     2|\n|  2|    a|     2|\n|  3|    a|     3|\n|  4|    a|     1|\n|  5|    b|     1|\n|  6|    b|     2|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the original columns to return\n",
    "df_filled1 = df_bfill.select(\"id\", \"value\", \"value2\")\n",
    "df_filled1.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DF Convert Aws Athena Code in Databricks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
